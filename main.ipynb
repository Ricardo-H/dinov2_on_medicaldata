{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,auc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from functools import partial\n",
    "from dinov2.eval.linear import LinearClassifier\n",
    "from dinov2.eval.utils import ModelWithIntermediateLayers\n",
    "from dinov2.eval.linear import create_linear_input\n",
    "#from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'my_network': 'dino1',\n",
    "    'dino_model': 'dinov2_vits14',\n",
    "    'pretrain_choice': 'unfrozen',\n",
    "    'num_cls': 2,\n",
    "    'dropout': 0.5,\n",
    "    'out_is_feature': True,\n",
    "    'use_patch_tokens': True,\n",
    "    'use_blocks':[3,6,9,12],        # 融合使用从输入开始的第几个block的特征\n",
    "\n",
    "    'lr': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'epoch': 50,                    # 学习率重启的周期:30,70,150,310\n",
    "    'num_set': 1,\n",
    "    'seed': 114514,\n",
    "    'img_size': 224,\n",
    "    'num_workers': 4,               # 4 or 11\n",
    "\n",
    "    'loss': 'cross_entropy',\n",
    "    'optimizer': 'sgd',            # adam, sgd\n",
    "    'scheduler': 'None',            # None,cosine, step\n",
    "    'scheduler_step_size': 20,\n",
    "\n",
    "    'device': 'cuda:0',\n",
    "    'save_dir': '/mnt/ssd/HUANGYouling/result/604',\n",
    "    'root_dir': '/mnt/ssd/HUANGYouling/medical_dataset',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取cfg,并且应用\n",
    "num_classes = cfg['num_cls']\n",
    "img_size = cfg['img_size']\n",
    "batch_size = cfg['batch_size']\n",
    "epoch = cfg['epoch']\n",
    "num_set = cfg['num_set']       # 用一个set或者用全部的10个\n",
    "learning_rate = cfg['lr']\n",
    "seed = cfg['seed']\n",
    "pretrain_choice = cfg['pretrain_choice']\n",
    "model_name = cfg['dino_model']\n",
    "\n",
    "\n",
    "root_dir = cfg['root_dir']\n",
    "data_name = f'CT_sets_{num_set}'\n",
    "\n",
    "save_dir = cfg['save_dir']\n",
    "dino_model_name = cfg['dino_model']\n",
    "\n",
    "blocks = cfg['use_blocks']\n",
    "patchtokens = cfg['use_patch_tokens']\n",
    "\n",
    "print(f'model_name: {model_name},\\npretrain_choice: {pretrain_choice},\\nlearning rate: {learning_rate},\\nbach_size: {batch_size},\\n\\\n",
    "epoch: {epoch},\\nnum_set: {num_set}')\n",
    "\n",
    "device = torch.device(cfg['device'])\n",
    "my_datetime = (datetime.now()+timedelta(hours=9)).strftime('%m-%d_%H:%M')\n",
    "save_name = f'{model_name}_blocks({blocks}_patchtokens{patchtokens}_param{pretrain_choice}_{data_name}_{my_datetime}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(cfg['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置输出文件信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# 结果保存信息\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "#writer = SummaryWriter('/mnt/ssd/HUANGYouling/logs', comment=save_name)\n",
    "\n",
    "# 定义文件名和路径\n",
    "output_file_name = f\"{save_path}/{datetime.now().strftime('%d')}/output_file_{my_datetime}.txt\"\n",
    "\n",
    "# 打开文件并将标准输出重定向到文件\n",
    "def redirect_stdout_to_file(file_name):\n",
    "    file = open(file_name, \"a\")  # 使用 \"a\" 模式以追加的方式打开文件\n",
    "    sys.stdout = file  # 将标准输出重定向到文件\n",
    "    return file\n",
    "\n",
    "# 恢复标准输出\n",
    "def restore_stdout(file):\n",
    "    sys.stdout = sys.__stdout__  # 恢复标准输出\n",
    "    file.close()  # 关闭文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root_dir = root\n",
    "        self.transform = transform\n",
    "        self.file_list = self.get_file_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        data = torch.load(file_path)  # 加载.pth文件\n",
    "        image = data['image']   \n",
    "        clinical = data['clinical']\n",
    "        label = data['label'].astype('float32')\n",
    "\n",
    "        # Convert each grayscale image to 3-channel RGB\n",
    "        nc_image = Image.fromarray(image[:,:,0].astype('uint8')).convert('RGB')\n",
    "        art_image = Image.fromarray(image[:,:,1].astype('uint8')).convert('RGB')\n",
    "        pv_image = Image.fromarray(image[:,:,2].astype('uint8')).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            nc_image = self.transform(nc_image)\n",
    "            art_image = self.transform(art_image)\n",
    "            pv_image = self.transform(pv_image)\n",
    "\n",
    "        return {'NC': nc_image, 'ART': art_image, 'PV': pv_image, 'clinical': clinical, 'label': label}\n",
    "\n",
    "    def get_file_list(self):\n",
    "        file_list = []\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pth'):\n",
    "                    file_list.append(file)\n",
    "        return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "      [\n",
    "      transforms.RandomResizedCrop(img_size),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.152, 0.233, 0.305], std=[0.063, 0.068, 0.089]) # 在全数据集上计算得到\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "      transforms.Resize(img_size),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.152, 0.233, 0.305], std=[0.063, 0.068, 0.089])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型1 \n",
    "1. 通过cfg['use_patch_tokens']来判断是否使用patch tokens [num_patch=256, dims=384]\n",
    "2. `neck_1结构`: 池化patch tokens,并且调整形状对齐cls后, 与 cls token [1,dims=384] concat后得到feature\n",
    "3. feature通过一个linear层分类  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithIntermediateLayers(nn.Module):\n",
    "    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n",
    "        super().__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_model.eval()\n",
    "        self.n_last_blocks = n_last_blocks\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.inference_mode():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.feature_model.get_intermediate_layers(\n",
    "                    images, self.n_last_blocks, return_class_token=True\n",
    "                )\n",
    "        return features\n",
    "\n",
    "\n",
    "def neck_1(x_tokens_list, use_n_blocks=1, use_avgpool=True):\n",
    "    # use_avgpool: 选择是否使用 patch token 的平均值 , 与 cls token 拼接后输出为feature\n",
    "    # 并且拼接之后feature token的长度是 cls token的两倍\n",
    "    patchtokens = x_tokens_list[-use_n_blocks][0]  # 选择倒数第几层的输出\n",
    "    cls = x_tokens_list[-use_n_blocks][1]         \n",
    "    output = cls\n",
    "    if use_avgpool:\n",
    "        output = torch.cat( (  output, torch.mean(patchtokens, dim=1),), dim=-1, )   # patch tokens\n",
    "        output = output.reshape(output.shape[0], -1)       # 将feature的最后两个维度展平,[2,dims]->[2*dims]\n",
    "    return output.float()\n",
    "\n",
    "class dino1(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(dino1, self).__init__()\n",
    "        model = torch.hub.load('facebookresearch/dinov2', cfg['dino_model'], pretrained=True)\n",
    "\n",
    "        autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=torch.float16)\n",
    "        self.tokens_list_extractor = ModelWithIntermediateLayers(model, n_last_blocks=list(range(12)), autocast_ctx=autocast_ctx) # 提取所有12层的输出\n",
    "        self.use_patch_tokens = cfg['use_patch_tokens']\n",
    "        self.use_blocks = cfg['use_blocks']\n",
    "\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.input_linear = model.embed_dim * 3\n",
    "        if cfg['use_patch_tokens']:\n",
    "            self.input_linear *= 2         # 经过拼接之后的feature长度是cls的2倍\n",
    "        self.input_linear *= len(self.use_blocks)             # 4个层的feature拼接 \n",
    "        self.linear = nn.Linear(self.input_linear, cfg['num_cls'])\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        full_block_tokens_turple = self.tokens_list_extractor(x)        \n",
    "        # 上一行full_block_tokens_turple是个[12,2]的turple矩阵\n",
    "        # 第0层对应vit的第12层,每一个元素是一个tensor\n",
    "        # tokens_turple[0,:] = [ patch_tokens [batch_size=32, num_patchs=256, dims=384], cls_token[patch_size, dims]  ]\n",
    "        feature = []\n",
    "        for n_block in self.use_blocks:\n",
    "            n_last_block = 13-n_block                       # 将正序号转换为倒序号\n",
    "            one_layer_block_tokens = neck_1(full_block_tokens_turple, use_n_blocks=n_last_block, use_avgpool=self.use_patch_tokens) \n",
    "            # 上一行代码将patch tokens与clstoken连接\n",
    "            # one_layer_block_tokens的shape是[batch_size, 2*dims]\n",
    "            feature.append(one_layer_block_tokens)\n",
    "        tensor_feature = torch.cat((feature), dim=1)\n",
    "        return tensor_feature\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f_NC = self.extract_feature(x1)  # (batch_size, embed_dim )=(1, 384))\n",
    "        f_ART = self.extract_feature(x2)\n",
    "        f_PV = self.extract_feature(x3)\n",
    "        feature = torch.cat((f_NC, f_ART, f_PV), dim=1)\n",
    "        pre = self.linear(feature)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型2  (待完成)\n",
    "与模型1不同点在于使用 `neck2结构`,使用卷积操作来降维patch tokens,然后与cls token concat后得到feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None):  # kernel, padding\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(c2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class neck_2(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(neck_2, self).__init__()\n",
    "        self.conv1d = Conv1d(c_in, c_out, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x_tokens_list, use_n_blocks=1, use_patch_tokens=True):\n",
    "        tokens = x_tokens_list[-use_n_blocks:]\n",
    "        cls = torch.cat([class_token for _, class_token in tokens], dim=-1)  # cls token\n",
    "\n",
    "        if use_patch_tokens:\n",
    "            patch_tokens = tokens[-1][0]  # 取最后一层的 patch tokens\n",
    "            # 通过 1x1 卷积\n",
    "            conv_output = self.conv1d(patch_tokens.transpose(1, 2)).transpose(1, 2)\n",
    "            output = torch.cat((cls, conv_output.view(conv_output.size(0), -1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)                # 维度变换为(batch_size, dims)，用于 fc 层之前\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class dino2(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(dino2, self).__init__()\n",
    "        model = torch.hub.load('facebookresearch/dinov2', cfg['dino_model'], pretrained=True)\n",
    "\n",
    "        autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=torch.float16)\n",
    "        self.tokens_list_extractor = ModelWithIntermediateLayers(model, n_last_blocks=1, autocast_ctx=autocast_ctx)\n",
    "        self.use_avgpool = cfg['use_patch_tokens']\n",
    "        self.neck_2 = neck_2(model.embed_dim * 2, model.embed_dim)\n",
    "\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.input_linear = model.embed_dim * 3\n",
    "        if cfg['use_patch_tokens']:\n",
    "            self.input_linear *= 2         # 经过拼接之后的feature长度是cls的2倍\n",
    "\n",
    "        self.linear = nn.Linear(self.input_linear, cfg['num_cls'])\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        tokens_list = self.tokens_list_extractor(x)\n",
    "        feature = neck_2(tokens_list, use_n_blocks=1, use_avgpool=self.use_avgpool)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.extract_feature(x1)  # (batch_size, embed_dim )=(1, 384))\n",
    "        f2 = self.extract_feature(x2)\n",
    "        f3 = self.extract_feature(x3)\n",
    "        feature = torch.cat((f1, f2, f3), dim=1)\n",
    "\n",
    "        pre = self.linear(feature)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet50(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(resnet50, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, cfg['num_cls'])\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        pre1 = self.model(x1)\n",
    "        pre2 = self.model(x2)\n",
    "        pre3 = self.model(x3)\n",
    "        return pre1, pre2, pre3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练设定 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_experiment(cfg, model, initial_model_path, device):\n",
    "    model.load_state_dict(torch.load(initial_model_path))\n",
    "    model.to(device)\n",
    "\n",
    "    if cfg['loss'] == 'cross_entropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if cfg['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=1e-4)\n",
    "    elif cfg['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=0)\n",
    "\n",
    "    if cfg['scheduler'] == 'cosine':\n",
    "        #scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10,T_mult=2, eta_min=1e-6)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, max_iter=epoch*690, eta_min=0)\n",
    "    elif cfg['scheduler'] == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "if cfg['my_network'] == 'dino1':\n",
    "    model = dino1(cfg).to(device)\n",
    "elif cfg['my_network'] == 'dino2':\n",
    "    model = dino2(cfg).to(device)\n",
    "\n",
    "#model = nn.DataParallel(model)              # 多GPU并行\n",
    "\n",
    "initial_model_path = f'/mnt/ssd/HUANGYouling/temp_model/{save_name}.pth'\n",
    "torch.save(model.state_dict(), initial_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok=True)\n",
    "#输出模型设置以及结果\n",
    "output_file_name = f\"{save_path}/output_file_{my_datetime}.txt\"\n",
    "output_file = redirect_stdout_to_file(output_file_name)\n",
    "out_print = '\\n'.join([f'{key}: {value}' for key, value in cfg.items()])\n",
    "print(out_print)\n",
    "restore_stdout(output_file)\n",
    "output_file.close()\n",
    "print(out_print)\n",
    "\n",
    "# 定义存储结果的列表\n",
    "set_train_loss_list = []\n",
    "set_train_acc_list = []\n",
    "set_val_loss_list = []\n",
    "set_val_acc_list = []\n",
    "set_auc_list = []\n",
    "\n",
    "for set_id in range(num_set):  # 数据集有 10 个 set\n",
    "    # 每个set开始时复原模型参数和实验设定\n",
    "    model, criterion, optimizer, scheduler = initialize_experiment(cfg, model, initial_model_path, device)\n",
    "\n",
    "    train_dataset = MedicalDataset(root=f'{root_dir}/{set_id}/train', transform=train_transform)\n",
    "    val_dataset = MedicalDataset(root=f'{root_dir}/{set_id}/test', transform=val_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        file = redirect_stdout_to_file(output_file_name)\n",
    "        print('-'*5, f'Set [{set_id+1}/{num_set}] Epoch [{i+1}/{epoch}] stared', '-'*5)\n",
    "        restore_stdout(file)\n",
    "\n",
    "        epoch_loss_list = []\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        epoch_acc_list = []\n",
    "        epoch_auc = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            images_NC, image_art, image_pv, targets = batch['NC'].to(device), batch['ART'].to(device), batch['PV'].to(device), batch['label'].to(device)\n",
    "            outputs = model(images_NC, image_art, image_pv)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            epoch_acc_list.append((outputs.argmax(dim=1) == targets.argmax(dim=1)).float().mean())\n",
    "            epoch_loss_list.append(loss.item() / targets.size(0))              # 计算这个batch_size的平均loss\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch_train_acc = sum(epoch_acc_list) / len(epoch_acc_list)\n",
    "        epoch_train_loss = sum(epoch_loss_list) / len(epoch_loss_list)\n",
    "\n",
    "        # 更新学习率\n",
    "        epoch_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:        \n",
    "            epoch_lr = scheduler.get_last_lr()[0]   \n",
    "            scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_loss_list = []\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            epoch_acc_list = []\n",
    "            epoch_auc = 0\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            all_targets = []\n",
    "            all_probabilities = []\n",
    "\n",
    "            for batch in val_loader:\n",
    "                images_NC, image_art, image_pv, targets = batch['NC'].to(device), batch['ART'].to(device), batch['PV'].to(device), batch['label'].to(device)\n",
    "                outputs = model(images_NC, image_art, image_pv)      \n",
    "     \n",
    "                # 计算acc和loss\n",
    "                loss = criterion(outputs, targets)\n",
    "                epoch_acc_list.append((outputs.argmax(dim=1) == targets.argmax(dim=1)).float().mean())   # 计算批次内正确预测的样本数\n",
    "                epoch_loss_list.append(loss.item() / targets.size(0))  \n",
    "\n",
    "                # 计算AUC\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                all_targets.extend(targets[:, 0].cpu().detach().numpy())  # 将每个batch的targets拼接起来\n",
    "                all_probabilities.extend(probabilities[:, 0].cpu().detach().numpy())  # 将每个batch的probabilities拼接起来\n",
    "\n",
    "            # 计算整个验证集上的平均准确率、损失和AUC\n",
    "            epoch_val_acc = sum(epoch_acc_list) / len(epoch_acc_list)\n",
    "            epoch_val_loss = sum(epoch_loss_list) / len(epoch_loss_list)   \n",
    "            # 计算AUC\n",
    "            auc = roc_auc_score(all_targets, all_probabilities, multi_class='ovr')\n",
    "\n",
    "            \n",
    "        # 输出每个epoch的结果\n",
    "        out_print = f' Set [{set_id+1}/{num_set}], epoch: [{i+1}/{epoch}], train_acc: {epoch_train_acc:.4f}, train_loss: {epoch_train_loss:.4f},\\\n",
    "        val_acc: {epoch_val_acc:.4f}, val_loss: {epoch_val_loss:.4f},auc: {auc:.4f}, LR: {epoch_lr:e}'\n",
    "        print(out_print)\n",
    "\n",
    "        file = redirect_stdout_to_file(output_file_name)\n",
    "        print(out_print)\n",
    "        restore_stdout(file)\n",
    "\n",
    "\n",
    "        train_loss_list.append(epoch_train_loss)\n",
    "        train_acc_list.append(epoch_train_acc)\n",
    "        val_loss_list.append(epoch_val_loss)\n",
    "        val_acc_list.append(epoch_val_acc)\n",
    "        auc_list.append(auc)\n",
    "    \n",
    "    # 保存train_loss最低的验证结果\n",
    "    epoch_index = train_loss_list.index(min(train_loss_list))\n",
    "\n",
    "    # 记录每个 set 最后一个epoch的结果,\n",
    "    set_train_loss_list.append(train_loss_list[epoch_index])\n",
    "    set_train_acc_list.append(train_acc_list[epoch_index])\n",
    "    set_val_acc_list.append(val_acc_list[epoch_index])\n",
    "    set_val_loss_list.append(val_loss_list[epoch_index])\n",
    "    set_auc_list.append(auc_list[epoch_index])\n",
    "\n",
    "    # 输出每个set 的结果\n",
    "    out_print = f'\\n Set [{set_id+1}/{num_set}] result:\\n 选取了每个epoch中train_loss最低的模型:train_acc: {set_train_acc_list[-1]:.4f}, train_loss: {set_train_loss_list[-1]:.4f},\\\n",
    "    val_acc: {set_val_acc_list[-1]:.4f}, val_loss: {set_val_loss_list[-1]:.4f},val_auc: {set_auc_list[-1]:.4f}\\n'\n",
    "    print(out_print)\n",
    "    file = redirect_stdout_to_file(output_file_name)\n",
    "    print(out_print)\n",
    "    restore_stdout(file)\n",
    "\n",
    "# 输出整个 10 个 set 的平均结果\n",
    "avg_train_loss = sum(set_train_loss_list) / len(set_train_loss_list)\n",
    "avg_train_acc = sum(set_train_acc_list) / len(set_train_acc_list)\n",
    "avg_val_loss = sum(set_val_loss_list) / len(set_val_loss_list)\n",
    "avg_val_acc = sum(set_val_acc_list) /  len(set_val_acc_list)\n",
    "avg_auc = sum(set_auc_list) / len(set_auc_list)\n",
    "\n",
    "out_print = '-'*100+f' \\n 选取了每个epoch中train_loss最低的模型: \\n avg_train_acc: {avg_train_acc:.4f}, avg_train_loss: {avg_train_loss:.4f},\\\n",
    "avg_val_acc: {avg_val_acc:.4f}, avg_val_loss: {avg_val_loss:.4f}, avg_auc: {avg_auc:.4f}'\n",
    "print(out_print)\n",
    "\n",
    "file = redirect_stdout_to_file(output_file_name)\n",
    "print(out_print)\n",
    "restore_stdout(file)\n",
    "\n",
    "torch.save(model, save_path+'/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習曲線の描画\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "# Define x_arr (the x-axis values) as the number of epochs\n",
    "x_arr = range(1, epoch + 1)\n",
    "\n",
    "for i in range(epoch):\n",
    "  train_acc.append(train_acc_list[i].cpu().detach().numpy())\n",
    "  train_loss.append(train_loss_list[i])\n",
    "  val_acc.append(val_acc_list[i].cpu().detach().numpy())\n",
    "  val_loss.append(val_loss_list[i])\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "ax[0].plot(x_arr, train_loss, '-o', label='Train loss')\n",
    "ax[0].plot(x_arr, val_loss, '--<', label='Validation loss')\n",
    "ax[0].legend(fontsize=15)\n",
    "ax[0].set_xlabel('Epoch', size=15)\n",
    "ax[0].set_ylabel('Loss', size=15)\n",
    "\n",
    "# Plot the training and validation accuracy on the second subplot\n",
    "ax[1].plot(x_arr, train_acc, '-o', label='Train accuracy')\n",
    "ax[1].plot(x_arr, val_acc, '--<', label='Validation accuracy')\n",
    "ax[1].legend(fontsize=15)\n",
    "ax[1].set_xlabel('Epoch', size=15)\n",
    "ax[1].set_ylabel('Accuracy', size=15)\n",
    "\n",
    "\n",
    "plt.savefig(f'{save_path}/learning_curve.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 绘制ROC曲线图\n",
    "# fpr, tpr, _ = roc_curve(all_targets, all_probabilities)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.savefig(save_path+f'/ROC_Curve.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全参数冻结,在整个数据集上验证"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
