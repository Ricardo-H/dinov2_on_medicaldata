{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "#from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'my_network': 'resnet50',\n",
    "    'dino_model': 'dinov2_vits14',\n",
    "    'pretrain_choice': 'unfrozen',\n",
    "    'num_cls': 2,\n",
    "    'dropout': 0.5,\n",
    "    'out_is_feature': True,\n",
    "    'use_patch_tokens': True,\n",
    "    'use_blocks':[3,6,9,12],        # 融合使用从输入开始的第几个block的特征\n",
    "\n",
    "    'memo': 'use_oneimg[1,2,3]_',\n",
    "    'print_to_file': False,         # 是否将print输出到文件\n",
    "\n",
    "    'lr': 1e-4,\n",
    "    'batch_size': 16,\n",
    "    'epoch': 50,                    # 学习率重启的周期:30,70,150,310\n",
    "    'num_set': 10,\n",
    "    'seed': 114514,\n",
    "    'img_size': 224,\n",
    "    'num_workers': 8,               # 4 or 11\n",
    "\n",
    "    'loss': 'cross_entropy',\n",
    "    'optimizer': 'adam',            # adam, sgd\n",
    "    'scheduler': 'step',            # None,cosinewarm, cosine, step\n",
    "    'scheduler_step_size': 25,\n",
    "\n",
    "    'device': 'cuda:0',\n",
    "    'save_dir': '/mnt/ssd/HUANGYouling/result/607',\n",
    "    'root_dir': '/mnt/ssd/HUANGYouling/medical_dataset',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取cfg,并且应用\n",
    "num_classes = cfg['num_cls']\n",
    "img_size = cfg['img_size']\n",
    "batch_size = cfg['batch_size']\n",
    "epoch = cfg['epoch']\n",
    "num_set = cfg['num_set']       # 用一个set或者用全部的10个\n",
    "learning_rate = cfg['lr']\n",
    "seed = cfg['seed']\n",
    "pretrain_choice = cfg['pretrain_choice']\n",
    "model_name = cfg['dino_model']\n",
    "\n",
    "\n",
    "root_dir = cfg['root_dir']\n",
    "data_name = f'CT_sets_{num_set}'\n",
    "\n",
    "save_dir = cfg['save_dir']\n",
    "dino_model_name = cfg['dino_model']\n",
    "\n",
    "blocks = cfg['use_blocks']\n",
    "patchtokens = cfg['use_patch_tokens']\n",
    "memo = cfg['memo']\n",
    "\n",
    "print(f'model_name: {model_name},\\npretrain_choice: {pretrain_choice},\\nlearning rate: {learning_rate},\\nbach_size: {batch_size},\\n\\\n",
    "epoch: {epoch},\\nnum_set: {num_set}')\n",
    "\n",
    "device = torch.device(cfg['device'])\n",
    "my_datetime = (datetime.now()+timedelta(hours=9)).strftime('%m-%d_%H:%M')\n",
    "save_name = f'{memo}_{model_name}_blocks({blocks}_patchtokens{patchtokens}_param{pretrain_choice}_{data_name}_{my_datetime}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(cfg['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置输出文件信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# 结果保存信息\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "#writer = SummaryWriter('/mnt/ssd/HUANGYouling/logs', comment=save_name)\n",
    "\n",
    "# 定义文件名和路径\n",
    "output_file_name = f\"{save_path}/{datetime.now().strftime('%d')}/output_file_{my_datetime}.txt\"\n",
    "\n",
    "# 打开文件并将标准输出重定向到文件\n",
    "def redirect_stdout_to_file(file_name):\n",
    "    file = open(file_name, \"a\")  # 使用 \"a\" 模式以追加的方式打开文件\n",
    "    sys.stdout = file  # 将标准输出重定向到文件\n",
    "    return file\n",
    "\n",
    "# 恢复标准输出\n",
    "def restore_stdout(file):\n",
    "    sys.stdout = sys.__stdout__  # 恢复标准输出\n",
    "    file.close()  # 关闭文件\n",
    "\n",
    "# 输出和日志记录函数\n",
    "def log_and_print(message, file_name, print_to_file=cfg['print_to_file']):\n",
    "    print(message)\n",
    "    if print_to_file:\n",
    "        file = redirect_stdout_to_file(file_name)\n",
    "        print(message)\n",
    "        restore_stdout(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root_dir = root\n",
    "        self.transform = transform\n",
    "        self.file_list = self.get_file_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        data = torch.load(file_path)  # 加载.pth文件\n",
    "        image = data['image']   \n",
    "        clinical = data['clinical']\n",
    "        label = data['label'].astype('float32')\n",
    "        label = np.argmax(label)            # 将 one-hot 编码[1,0]的 label 转换为类别索引[0]\n",
    "\n",
    "        # Convert each grayscale image to 3-channel RGB\n",
    "        nc_image = Image.fromarray(image[:,:,0].astype('uint8')).convert('RGB')\n",
    "        art_image = Image.fromarray(image[:,:,1].astype('uint8')).convert('RGB')\n",
    "        pv_image = Image.fromarray(image[:,:,2].astype('uint8')).convert('RGB')\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            nc_image = self.transform(nc_image)\n",
    "            art_image = self.transform(art_image)\n",
    "            pv_image = self.transform(pv_image)\n",
    "\n",
    "        return {'image':image, 'NC': nc_image, 'ART': art_image, 'PV': pv_image, 'clinical': clinical, 'label': label}\n",
    "\n",
    "    def get_file_list(self):\n",
    "        file_list = []\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pth'):\n",
    "                    file_list.append(file)\n",
    "        return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "      [\n",
    "      transforms.CenterCrop(img_size),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.RandomRotation(degrees=15),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.152, 0.233, 0.305], std=[0.063, 0.068, 0.089]) # 在全数据集上计算得到\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "      transforms.CenterCrop(img_size),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.152, 0.233, 0.305], std=[0.063, 0.068, 0.089])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型1 \n",
    "1. 通过cfg['use_patch_tokens']来判断是否使用patch tokens [num_patch=256, dims=384]\n",
    "2. `neck_1结构`: 池化patch tokens,并且调整形状对齐cls后, 与 cls token [1,dims=384] concat后得到feature\n",
    "3. feature通过一个linear层分类  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithIntermediateLayers(nn.Module):\n",
    "    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n",
    "        super().__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_model.eval()\n",
    "        self.n_last_blocks = n_last_blocks\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.inference_mode():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.feature_model.get_intermediate_layers(\n",
    "                    images, self.n_last_blocks, return_class_token=True\n",
    "                )\n",
    "        return features\n",
    "\n",
    "\n",
    "def neck_1(x_tokens_list, use_n_blocks=1, use_avgpool=True):\n",
    "    # use_avgpool: 选择是否使用 patch token 的平均值 , 与 cls token 拼接后输出为feature\n",
    "    # 并且拼接之后feature token的长度是 cls token的两倍\n",
    "    patchtokens = x_tokens_list[-use_n_blocks][0]  # 选择倒数第几层的输出\n",
    "    cls = x_tokens_list[-use_n_blocks][1]         \n",
    "    output = cls\n",
    "    if use_avgpool:\n",
    "        output = torch.cat( (  output, torch.mean(patchtokens, dim=1),), dim=-1, )   # patch tokens\n",
    "        output = output.reshape(output.shape[0], -1)       # 将feature的最后两个维度展平,[2,dims]->[2*dims]\n",
    "    return output.float()\n",
    "\n",
    "class dino1(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(dino1, self).__init__()\n",
    "        model = torch.hub.load('facebookresearch/dinov2', cfg['dino_model'], pretrained=True)\n",
    "\n",
    "        autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=torch.float16)\n",
    "        self.tokens_list_extractor = ModelWithIntermediateLayers(model, n_last_blocks=list(range(12)), autocast_ctx=autocast_ctx) # 提取所有12层的输出\n",
    "        self.use_patch_tokens = cfg['use_patch_tokens']\n",
    "        self.use_blocks = cfg['use_blocks']\n",
    "\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.input_linear = model.embed_dim * 3\n",
    "        if cfg['use_patch_tokens']:\n",
    "            self.input_linear *= 2         # 经过拼接之后的feature长度是cls的2倍\n",
    "        self.input_linear *= len(self.use_blocks)             # 4个层的feature拼接 \n",
    "        self.linear = nn.Linear(self.input_linear, cfg['num_cls'])\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        full_block_tokens_turple = self.tokens_list_extractor(x)        \n",
    "        # 上一行full_block_tokens_turple是个[12,2]的turple矩阵\n",
    "        # 第0层对应vit的第12层,每一个元素是一个tensor\n",
    "        # tokens_turple[0,:] = [ patch_tokens [batch_size=32, num_patchs=256, dims=384], cls_token[patch_size, dims]  ]\n",
    "        feature = []\n",
    "        for n_block in self.use_blocks:\n",
    "            n_last_block = 13-n_block                       # 将正序号转换为倒序号\n",
    "            one_layer_block_tokens = neck_1(full_block_tokens_turple, use_n_blocks=n_last_block, use_avgpool=self.use_patch_tokens) \n",
    "            # 上一行代码将patch tokens与clstoken连接\n",
    "            # one_layer_block_tokens的shape是[batch_size, 2*dims]\n",
    "            feature.append(one_layer_block_tokens)\n",
    "        tensor_feature = torch.cat((feature), dim=1)\n",
    "        return tensor_feature\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f_NC = self.extract_feature(x1)  # (batch_size, embed_dim )=(1, 384))\n",
    "        f_ART = self.extract_feature(x2)\n",
    "        f_PV = self.extract_feature(x3)\n",
    "        feature = torch.cat((f_NC, f_ART, f_PV), dim=1)\n",
    "        pre = self.linear(feature)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型2  (待完成)\n",
    "与模型1不同点在于使用 `neck2结构`,使用卷积操作来降维patch tokens,然后与cls token concat后得到feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None):  # kernel, padding\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(c2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class neck_2(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(neck_2, self).__init__()\n",
    "        self.conv1d = Conv1d(c_in, c_out, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x_tokens_list, use_n_blocks=1, use_patch_tokens=True):\n",
    "        tokens = x_tokens_list[-use_n_blocks:]\n",
    "        cls = torch.cat([class_token for _, class_token in tokens], dim=-1)  # cls token\n",
    "\n",
    "        if use_patch_tokens:\n",
    "            patch_tokens = tokens[-1][0]  # 取最后一层的 patch tokens\n",
    "            # 通过 1x1 卷积\n",
    "            conv_output = self.conv1d(patch_tokens.transpose(1, 2)).transpose(1, 2)\n",
    "            output = torch.cat((cls, conv_output.view(conv_output.size(0), -1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)                # 维度变换为(batch_size, dims)，用于 fc 层之前\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class dino2(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(dino2, self).__init__()\n",
    "        model = torch.hub.load('facebookresearch/dinov2', cfg['dino_model'], pretrained=True)\n",
    "\n",
    "        autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=torch.float16)\n",
    "        self.tokens_list_extractor = ModelWithIntermediateLayers(model, n_last_blocks=1, autocast_ctx=autocast_ctx)\n",
    "        self.use_avgpool = cfg['use_patch_tokens']\n",
    "        self.neck_2 = neck_2(model.embed_dim * 2, model.embed_dim)\n",
    "\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.input_linear = model.embed_dim * 3\n",
    "        if cfg['use_patch_tokens']:\n",
    "            self.input_linear *= 2         # 经过拼接之后的feature长度是cls的2倍\n",
    "\n",
    "        self.linear = nn.Linear(self.input_linear, cfg['num_cls'])\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        tokens_list = self.tokens_list_extractor(x)\n",
    "        feature = neck_2(tokens_list, use_n_blocks=1, use_avgpool=self.use_avgpool)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.extract_feature(x1)  # (batch_size, embed_dim )=(1, 384))\n",
    "        f2 = self.extract_feature(x2)\n",
    "        f3 = self.extract_feature(x3)\n",
    "        feature = torch.cat((f1, f2, f3), dim=1)\n",
    "\n",
    "        pre = self.linear(feature)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline from Wang weibin\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet50(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(resnet50, self).__init__()\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.linearinput = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(self.linearinput, cfg['num_cls'])\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        f_1 = self.feature_extractor(x1)\n",
    "        # f_2 = self.feature_extractor(x2)\n",
    "        # f_3 = self.feature_extractor(x3)\n",
    "        #feature = torch.cat((f_1, f_2, f_3), dim=1)\n",
    "        pre = self.fc(f_1)\n",
    "        return pre\n",
    "\n",
    "class resnet18(nn.Module):\n",
    "    def __init__(self, cfg=None):\n",
    "        super(resnet18, self).__init__()\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        if cfg['pretrain_choice'] == 'frozen':\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.linearinput = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(self.linearinput, cfg['num_cls'])\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        f_1 = self.feature_extractor(x1)\n",
    "        # f_2 = self.feature_extractor(x2)\n",
    "        # f_3 = self.feature_extractor(x3)\n",
    "        #feature = torch.cat((f_1, f_2, f_3), dim=1)\n",
    "        pre = self.fc(f_1)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok=True)\n",
    "#输出模型设置以及结果\n",
    "output_file_name = f\"{save_path}/output_file_{my_datetime}.txt\"\n",
    "output_file = redirect_stdout_to_file(output_file_name)\n",
    "out_print = '\\n'.join([f'{key}: {value}' for key, value in cfg.items()])\n",
    "log_and_print(out_print, output_file_name)\n",
    "\n",
    "# 定义存储结果的列表\n",
    "set_train_loss_list,set_train_acc_list,set_val_loss_list,set_val_acc_list, set_val_auc_list, \\\n",
    "                set_test_auc_list,set_test_loss_list,set_test_acc_list = [],[],[],[],[],[],[],[]\n",
    "\n",
    "# 网络模型的初始化\n",
    "def get_save_model(cfg):\n",
    "    if cfg['my_network'] == 'dino1':\n",
    "        model = dino1(cfg).to(device)\n",
    "    elif cfg['my_network'] == 'dino2':\n",
    "        model = dino2(cfg).to(device)\n",
    "    elif cfg['my_network'] == 'resnet50':\n",
    "        model = resnet50(cfg).to(device)\n",
    "    elif cfg['my_network'] == 'resnet18':\n",
    "        model = resnet18(cfg).to(device)\n",
    "    initial_model_path = f'/mnt/ssd/HUANGYouling/temp_model/{save_name}.pth'        # 为了每个set开始时都能复原模型参数,临时保存初始参数\n",
    "    torch.save(model.state_dict(), initial_model_path)\n",
    "    return model, initial_model_path\n",
    "# 损失函数,优化器,学习率调度器的初始化\n",
    "def initialize_experiment(cfg, model, initial_model_path, device):\n",
    "    model.load_state_dict(torch.load(initial_model_path))\n",
    "    model.to(device)\n",
    "\n",
    "    if cfg['loss'] == 'cross_entropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if cfg['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=1e-4)\n",
    "    elif cfg['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=0)\n",
    "\n",
    "    if cfg['scheduler'] == 'cosinewarm':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10,T_mult=2, eta_min=1e-6)\n",
    "    elif cfg['scheduler'] == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['epoch'], eta_min=0)\n",
    "    elif cfg['scheduler'] == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=cfg['scheduler_step_size'], gamma=0.1)\n",
    "    elif cfg['scheduler'] == 'None':\n",
    "        scheduler = None\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n",
    "# 每一次实验set的数据集的初始化\n",
    "def initial_dataloader(cfg,root_dir,set_id,train_transform,val_transform):\n",
    "    full_train_dataset = MedicalDataset(root=f'{root_dir}/{set_id}/train', transform=train_transform)\n",
    "    train_dataset, val_dataset = train_test_split(full_train_dataset, test_size=0.2, random_state=42)   # 8:2划分训练集和验证集\n",
    "    test_dataset = MedicalDataset(root=f'{root_dir}/{set_id}/test', transform=val_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=cfg['num_workers'], pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_one_epoch(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # images_NC, image_art, image_pv, targets = batch['NC'].to(device), batch['ART'].to(device), batch['PV'].to(device), batch['label'].to(device)\n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = model(images_NC, image_art, image_pv)\n",
    "\n",
    "        images, targets = batch['image'].to(device), batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == targets)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)   # 使用train_dataset的长度\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # images_NC, image_art, image_pv, targets = batch['NC'].to(device), batch['ART'].to(device), batch['PV'].to(device), batch['label'].to(device)\n",
    "            # outputs = model(images_NC, image_art, image_pv)\n",
    "            images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            all_targets.extend(labels.cpu().detach().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().detach().numpy())\n",
    "            \n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(loader.dataset)\n",
    "    # 提取正类的概率\n",
    "    all_probabilities_1d = np.array([prob[1] for prob in all_probabilities])\n",
    "    epoch_auc = roc_auc_score(all_targets, all_probabilities_1d, multi_class='ovr')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_auc\n",
    "\n",
    "model, initial_model_path = get_save_model(cfg)         #设定并且保存模型参数\n",
    "\n",
    "for set_id in range(num_set):  # 数据集有 10 个 set\n",
    "    # 每个set开始时复原模型参数和实验设定\n",
    "    model, criterion, optimizer, scheduler = initialize_experiment(cfg, model, initial_model_path, device)\n",
    "    #model = nn.DataParallel(model)              # 多GPU并行\n",
    "    train_loader, val_loader, test_loader = initial_dataloader(cfg,root_dir, set_id, train_transform, val_transform)\n",
    "\n",
    "\n",
    "    train_loss_list, train_acc_list, val_loss_list, val_acc_list, val_auc_list, \\\n",
    "                test_auc_list, test_loss_list, test_acc_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "    for i in range(epoch):\n",
    "        file = redirect_stdout_to_file(output_file_name)\n",
    "        print('-'*5, f'Set [{set_id+1}/{num_set}] Epoch [{i+1}/{epoch}] stared', '-'*5)\n",
    "        restore_stdout(file)\n",
    "\n",
    "        epoch_train_loss, epoch_train_acc = train_one_epoch(train_loader, model, criterion, optimizer, device)  # 训练一个epoch        \n",
    "        epoch_val_loss, epoch_val_acc, epoch_val_auc = evaluate(val_loader, model, criterion, device)\n",
    "        epoch_test_loss, epoch_test_acc, epoch_test_auc = evaluate(test_loader, model, criterion, device)\n",
    "\n",
    "        # 更新学习率\n",
    "        if scheduler is not None:        \n",
    "            epoch_lr = scheduler.get_last_lr()[0]   \n",
    "            scheduler.step()\n",
    "        else:\n",
    "            epoch_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            \n",
    "        # 输出每个 epoch 的结果\n",
    "        epoch_log = (f'Set [{set_id+1}/{num_set}], Epoch [{i+1}/{epoch}],\\n '\n",
    "                     f'Train Acc: {epoch_train_acc:.4f}, Train Loss: {epoch_train_loss:.4f},   '\n",
    "                     f'Val Acc: {epoch_val_acc:.4f}, Val Loss: {epoch_val_loss:.4f}, Val AUC: {epoch_val_auc:.4f},\\n '\n",
    "                     f'Test Acc: {epoch_test_acc:.4f}, Test Loss: {epoch_test_loss:.4f}, Test AUC: {epoch_test_auc:.4f}, LR: {epoch_lr:.6f}')\n",
    "        log_and_print(epoch_log, output_file_name)\n",
    "\n",
    "\n",
    "        train_loss_list.append(epoch_train_loss),   train_acc_list.append(epoch_train_acc) \n",
    "        val_loss_list.append(epoch_val_loss),       val_acc_list.append(epoch_val_acc), val_auc_list.append(epoch_val_auc)\n",
    "        test_loss_list.append(epoch_test_loss),     test_acc_list.append(epoch_test_acc), test_auc_list.append(epoch_test_auc)\n",
    "    \n",
    "    # 保存val_loss最低的验证结果\n",
    "    epoch_index = val_loss_list.index(min(val_loss_list))\n",
    "\n",
    "    # 记录每个 set 按策略选定的epoch的结果\n",
    "    set_train_loss_list.append(train_loss_list[epoch_index]), set_train_acc_list.append(train_acc_list[epoch_index])\n",
    "    set_val_acc_list.append(val_acc_list[epoch_index]), set_val_loss_list.append(val_loss_list[epoch_index])\n",
    "    set_test_acc_list.append(test_acc_list[epoch_index]), set_test_loss_list.append(test_loss_list[epoch_index])\n",
    "    set_val_auc_list.append(val_auc_list[epoch_index]), set_test_auc_list.append(test_auc_list[epoch_index])\n",
    "\n",
    "    # 输出每个 set 的结果\n",
    "    set_log = (f'\\nSet [{set_id+1}/{num_set}] result:\\n'\n",
    "               f'Best epoch based on mini-Val-Loss: \\n'\n",
    "               f'Train Acc: {train_acc_list[epoch_index]:.4f}, Train Loss: {train_loss_list[epoch_index]:.4f},\\n '\n",
    "               f'Val Acc: {val_acc_list[epoch_index]:.4f}, Val Loss: {val_loss_list[epoch_index]:.4f},  AUC: {val_auc_list[epoch_index]:.4f},\\n '\n",
    "               f'Test Acc: {test_acc_list[epoch_index]:.4f}, Test Loss: {test_loss_list[epoch_index]:.4f}, Test AUC: {test_auc_list[epoch_index]:.4f}\\n')\n",
    "    log_and_print(set_log, output_file_name)\n",
    "\n",
    "# 输出整个 10 个 set 的平均结果\n",
    "avg_train_loss = sum(set_train_loss_list) / len(set_train_loss_list)\n",
    "avg_train_acc = sum(set_train_acc_list) / len(set_train_acc_list)\n",
    "avg_val_loss = sum(set_val_loss_list) / len(set_val_loss_list)\n",
    "avg_val_acc = sum(set_val_acc_list) /  len(set_val_acc_list)\n",
    "avg_test_acc = sum(set_test_acc_list) / len(set_test_acc_list)\n",
    "avg_test_loss = sum(set_test_loss_list) / len(set_test_loss_list)\n",
    "avg_test_auc = sum(set_test_auc_list) / len(set_test_auc_list)\n",
    "\n",
    "final_log = (f'\\nOverall results across all sets:\\n'\n",
    "             f'Avg Train Acc: {avg_train_acc:.4f}, Avg Train Loss: {avg_train_loss:.4f}, '\n",
    "             f'Avg Val Acc: {avg_val_acc:.4f}, Avg Val Loss: {avg_val_loss:.4f}, Avg AUC: {avg_test_auc:.4f}'\n",
    "             f'Avg Test Acc: {avg_test_acc:.4f}, Avg Test Loss: {avg_test_loss:.4f}, Avg AUC: {avg_test_auc:.4f}\\n')\n",
    "log_and_print(final_log, output_file_name)\n",
    "\n",
    "torch.save(model, save_path+'/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習曲線の描画\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "x_arr = range(1, epoch + 1)\n",
    "for i in range(epoch):\n",
    "  train_acc.append(train_acc_list[i].cpu().detach().numpy())\n",
    "  train_loss.append(train_loss_list[i])\n",
    "  val_acc.append(val_acc_list[i].cpu().detach().numpy())\n",
    "  val_loss.append(val_loss_list[i])\n",
    "  test_acc.append(test_acc_list[i].cpu().detach().numpy())\n",
    "  test_loss.append(test_loss_list[i])\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "ax[0].plot(x_arr, train_loss, '-o', label='Train loss')\n",
    "ax[0].plot(x_arr, val_loss, '--<', label='Validation loss')\n",
    "ax[0].plot(x_arr, test_loss, '--<', label='Test loss')\n",
    "ax[0].legend(fontsize=8)\n",
    "ax[0].set_xlabel('Epoch', size=10)\n",
    "ax[0].set_ylabel('Loss', size=10)\n",
    "\n",
    "# Plot the training and validation accuracy on the second subplot\n",
    "ax[1].plot(x_arr, train_acc, '-o', label='Train accuracy')\n",
    "ax[1].plot(x_arr, val_acc, '--<', label='Validation accuracy')\n",
    "ax[1].plot(x_arr, test_acc, '--<', label='Test accuracy')\n",
    "ax[1].legend(fontsize=8)\n",
    "ax[1].set_xlabel('Epoch', size=10)\n",
    "ax[1].set_ylabel('Accuracy', size=10)\n",
    "\n",
    "plt.savefig(f'{save_path}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 绘制ROC曲线图\n",
    "# fpr, tpr, _ = roc_curve(all_targets, all_probabilities)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.savefig(save_path+f'/ROC_Curve.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全参数冻结,在整个数据集上验证"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
